{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸŽ“ **Professor**: Apostolos Filippas\n",
    "\n",
    "### ðŸ“˜ **Class**: AI Engineering\n",
    "\n",
    "### ðŸ“‹ **Homework 4**: Embeddings & Semantic Search\n",
    "\n",
    "### ðŸ“… **Due Date**: Day of Lecture 5, 11:59 PM\n",
    "\n",
    "\n",
    "**Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you'll build on Homework 3 (BM25 search) by adding **embedding-based semantic search**.\n",
    "\n",
    "You will:\n",
    "1. **Generate embeddings** using both local (Hugging Face) and API (OpenAI) models\n",
    "2. **Implement cosine similarity** from scratch\n",
    "3. **Implement semantic search** from scratch\n",
    "4. **Compare BM25 vs semantic search** using Recall\n",
    "5. **Compare different embedding models** and analyze their differences\n",
    "\n",
    "**Total Points: 95**\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Complete all tasks by filling in code where you see `# YOUR CODE HERE`\n",
    "- You may use ChatGPT, Claude, documentation, Stack Overflow, etc.\n",
    "- When using external resources, briefly cite them in a comment\n",
    "- Run all cells before submitting to ensure they work\n",
    "\n",
    "**Submission:**\n",
    "1. Create a branch called `homework-4`\n",
    "2. Commit and push your work\n",
    "3. Create a PR and merge to main\n",
    "4. Submit the `.ipynb` file on Blackboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1: Environment Setup (10 points)\n",
    "\n",
    "### 1a. Imports (5 pts)\n",
    "\n",
    "Import the required libraries and load the WANDS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: c:\\Users\\mohit\\ai-engineering-fordham\\ce\\homework\n",
      "Here: C:\\Users\\mohit\\ai-engineering-fordham\\ce\\homework\n",
      "Parent: C:\\Users\\mohit\\ai-engineering-fordham\\ce\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"Here:\", Path(\".\").resolve())\n",
    "print(\"Parent:\", Path(\"..\").resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helpers should be at: C:\\Users\\mohit\\ai-engineering-fordham\\ce\\scripts\\helpers.py\n",
      "exists? True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "p = (Path(\"..\") / \"scripts\" / \"helpers.py\").resolve()\n",
    "print(\"helpers should be at:\", p)\n",
    "print(\"exists?\", p.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base: C:\\Users\\mohit\\ai-engineering-fordham\n",
      "Found count: 7\n",
      "C:\\Users\\mohit\\ai-engineering-fordham\\ce\\scripts\\helpers.py\n",
      "C:\\Users\\mohit\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\aiohttp\\helpers.py\n",
      "C:\\Users\\mohit\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\pyparsing\\helpers.py\n",
      "C:\\Users\\mohit\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\setuptools\\tests\\integration\\helpers.py\n",
      "C:\\Users\\mohit\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\jedi\\api\\helpers.py\n",
      "C:\\Users\\mohit\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\jedi\\inference\\helpers.py\n",
      "C:\\Users\\mohit\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\aiohttp\\_websocket\\helpers.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base = Path.cwd().resolve().parents[1]  # ce/\n",
    "print(\"Base:\", base)\n",
    "\n",
    "found = list(base.rglob(\"helpers.py\"))\n",
    "print(\"Found count:\", len(found))\n",
    "for p in found[:20]:\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# ruff: noqa: E402\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import ONLY data loading from helpers\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "from helpers import load_wands_products, load_wands_queries, load_wands_labels\n",
    "\n",
    "# Embedding libraries - we use these directly\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import litellm\n",
    "\n",
    "# Load environment variables for API keys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products: 42,994\n",
      "Queries: 480\n",
      "Labels: 233,448\n"
     ]
    }
   ],
   "source": [
    "# Load the WANDS dataset\n",
    "products = load_wands_products()\n",
    "queries = load_wands_queries()\n",
    "labels = load_wands_labels()\n",
    "\n",
    "print(f\"Products: {len(products):,}\")\n",
    "print(f\"Queries: {len(queries):,}\")\n",
    "print(f\"Labels: {len(labels):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Copy BM25 functions from HW3 (5 pts)\n",
    "\n",
    "Copy your BM25 implementation from Homework 3. We'll use it to compare against semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API working!\n"
     ]
    }
   ],
   "source": [
    "# Task 1b: Verify API keys\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say 'API working!' and nothing else.\"}],\n",
    "    max_tokens=20\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions (provided)\n",
    "# Note: Data from WANDS (Wayfair Annotated Dataset)\n",
    "# Source: https://github.com/wayfair/WANDS\n",
    "\n",
    "def load_wands_products(data_dir: str = \"../data\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load WANDS products from local file.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Path to the data directory containing wayfair-products.csv\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with product information including product_id, product_name,\n",
    "        product_class, category_hierarchy, product_description, etc.\n",
    "    \"\"\"\n",
    "    filepath = Path(data_dir) / \"wayfair-products.csv\"\n",
    "    products = pd.read_csv(filepath, sep='\\t')\n",
    "    products = products.rename(columns={'category hierarchy': 'category_hierarchy'})\n",
    "    return products\n",
    "\n",
    "def load_wands_queries(data_dir: str = \"../data\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load WANDS queries from local file.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Path to the data directory containing wayfair-queries.csv\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with query_id and query columns\n",
    "    \"\"\"\n",
    "    filepath = Path(data_dir) / \"wayfair-queries.csv\"\n",
    "    queries = pd.read_csv(filepath, sep='\\t')\n",
    "    return queries\n",
    "\n",
    "def load_wands_labels(data_dir: str = \"../data\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load WANDS relevance labels from local file.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Path to the data directory containing wayfair-labels.csv\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with query_id, product_id, label (Exact/Partial/Irrelevant),\n",
    "        and grade (2/1/0) columns\n",
    "    \"\"\"\n",
    "    filepath = Path(data_dir) / \"wayfair-labels.csv\"\n",
    "    labels = pd.read_csv(filepath, sep='\\t')\n",
    "    grade_map = {'Exact': 2, 'Partial': 1, 'Irrelevant': 0}\n",
    "    labels['grade'] = labels['label'].map(grade_map)\n",
    "    return labels\n",
    "\n",
    "print(\"Loading functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Stemmer\n",
    "stemmer = Stemmer.Stemmer(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2a: Load the data\n",
    "\n",
    "# YOUR CODE HERE\n",
    "stemmer = Stemmer.Stemmer(\"english\")\n",
    "\n",
    "def stemming_tokenize(text: str):\n",
    "    # 1. lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # 3. split into tokens\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # 4. stem tokens\n",
    "    tokens = stemmer.stemWords(tokens)\n",
    "    \n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = load_wands_products()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter, defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 functions ready: snowball_tokenize, build_index, score_bm25, search_bm25\n"
     ]
    }
   ],
   "source": [
    "# --- Task 2: BM25 (self-contained cell) ---\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import Stemmer  # PyStemmer\n",
    "\n",
    "# Stemmer + punctuation translation\n",
    "stemmer = Stemmer.Stemmer(\"english\")\n",
    "punct_trans = str.maketrans({key: \" \" for key in string.punctuation})\n",
    "\n",
    "def snowball_tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Tokenize text with punctuation removal + lowercasing + stemming.\"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return []\n",
    "    text = str(text).translate(punct_trans)\n",
    "    tokens = text.lower().split()\n",
    "    return [stemmer.stemWord(token) for token in tokens]\n",
    "\n",
    "def build_index(docs: list[str], tokenizer) -> tuple[dict, list[int]]:\n",
    "    \"\"\"\n",
    "    Build an inverted index from a list of documents.\n",
    "    Returns (index, doc_lengths).\n",
    "    \"\"\"\n",
    "    index: dict[str, dict[int, int]] = {}\n",
    "    doc_lengths: list[int] = []\n",
    "\n",
    "    for doc_id, doc in enumerate(docs):\n",
    "        tokens = tokenizer(doc)\n",
    "        doc_lengths.append(len(tokens))\n",
    "        term_counts = Counter(tokens)\n",
    "\n",
    "        for term, count in term_counts.items():\n",
    "            if term not in index:\n",
    "                index[term] = {}\n",
    "            index[term][doc_id] = count\n",
    "\n",
    "    return index, doc_lengths\n",
    "\n",
    "def get_df(term: str, index: dict) -> int:\n",
    "    \"\"\"Document frequency of term.\"\"\"\n",
    "    return len(index.get(term, {}))\n",
    "\n",
    "def bm25_idf(df: int, num_docs: int) -> float:\n",
    "    \"\"\"BM25 IDF.\"\"\"\n",
    "    return np.log((num_docs - df + 0.5) / (df + 0.5) + 1)\n",
    "\n",
    "def bm25_tf(tf: int, doc_len: int, avg_doc_len: float, k1: float = 1.2, b: float = 0.75) -> float:\n",
    "    \"\"\"BM25 TF normalization.\"\"\"\n",
    "    return (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * doc_len / avg_doc_len))\n",
    "\n",
    "def score_bm25(\n",
    "    query: str,\n",
    "    index: dict,\n",
    "    num_docs: int,\n",
    "    doc_lengths: list[int],\n",
    "    tokenizer,\n",
    "    k1: float = 1.2,\n",
    "    b: float = 0.75\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Return BM25 scores for all docs.\"\"\"\n",
    "    query_tokens = tokenizer(query)\n",
    "    scores = np.zeros(num_docs)\n",
    "    avg_doc_len = float(np.mean(doc_lengths)) if doc_lengths else 1.0\n",
    "\n",
    "    for token in query_tokens:\n",
    "        df = get_df(token, index)\n",
    "        if df == 0:\n",
    "            continue\n",
    "\n",
    "        idf = bm25_idf(df, num_docs)\n",
    "\n",
    "        postings = index.get(token, {})\n",
    "        for doc_id, tf in postings.items():\n",
    "            tf_norm = bm25_tf(tf, doc_lengths[doc_id], avg_doc_len, k1, b)\n",
    "            scores[doc_id] += idf * tf_norm\n",
    "\n",
    "    return scores\n",
    "\n",
    "def search_products(\n",
    "    query: str,\n",
    "    products_df: pd.DataFrame,\n",
    "    index: dict,\n",
    "    doc_lengths: list[int],\n",
    "    tokenizer,\n",
    "    k: int = 10\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Return top-k rows from products_df with BM25 scores.\"\"\"\n",
    "    scores = score_bm25(query, index, len(products_df), doc_lengths, tokenizer)\n",
    "    top_k_idx = np.argsort(-scores)[:k]\n",
    "\n",
    "    results = products_df.iloc[top_k_idx].copy()\n",
    "    results[\"score\"] = scores[top_k_idx]\n",
    "    results[\"rank\"] = range(1, len(results) + 1)\n",
    "    return results\n",
    "\n",
    "# --- Wrapper to match HW4 expected function name/signature ---\n",
    "def search_bm25(\n",
    "    query: str,\n",
    "    index: dict,\n",
    "    products_df: pd.DataFrame,\n",
    "    doc_lengths: list[int],\n",
    "    tokenizer=snowball_tokenize,\n",
    "    k: int = 10\n",
    ") -> pd.DataFrame:\n",
    "    return search_products(query, products_df, index, doc_lengths, tokenizer, k)\n",
    "\n",
    "print(\"BM25 functions ready: snowball_tokenize, build_index, score_bm25, search_bm25\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Build BM25 index + run a test query (auto-picks text column)\n",
    "\n",
    "# 0) Pick the best text column automatically\n",
    "cols = products.columns.tolist()\n",
    "candidates = [c for c in cols if any(k in c.lower() for k in [\"title\", \"name\", \"desc\", \"text\"])]\n",
    "print(\"Text column candidates:\", candidates)\n",
    "\n",
    "if not candidates:\n",
    "    raise ValueError(f\"No obvious text column found. Available columns: {cols}\")\n",
    "\n",
    "TEXT_COL = candidates[0]  # pick the first reasonable match\n",
    "print(\"Using TEXT_COL =\", TEXT_COL)\n",
    "\n",
    "# 1) Extract docs\n",
    "docs = products[TEXT_COL].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "# 2) Build index\n",
    "index, doc_lengths = build_index(docs, snowball_tokenize)\n",
    "\n",
    "# 3) Test query\n",
    "q = queries.iloc[0][\"query\"] if hasattr(queries, \"iloc\") else queries[0]\n",
    "results = search_bm25(q, index, products, doc_lengths, k=10)\n",
    "\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'product_title'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohit\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3641\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3640\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3642\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:168\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:197\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7668\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7676\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'product_title'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 1. Extract text column\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m docs = \u001b[43mproducts\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mproduct_title\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.fillna(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).astype(\u001b[38;5;28mstr\u001b[39m).tolist()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 2. Build index\u001b[39;00m\n\u001b[32m      5\u001b[39m index, doc_lengths = build_index(docs, snowball_tokenize)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohit\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4378\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4376\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4377\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4378\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4380\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohit\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3648\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3643\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3644\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3645\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3646\u001b[39m     ):\n\u001b[32m   3647\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3648\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3649\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3650\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3651\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3652\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3653\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'product_title'"
     ]
    }
   ],
   "source": [
    "# 1. Extract text column\n",
    "docs = products[\"product_title\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "# 2. Build index\n",
    "index, doc_lengths = build_index(docs, snowball_tokenize)\n",
    "\n",
    "# 3. Test query\n",
    "q = queries.iloc[0][\"query\"]\n",
    "results = search_bm25(q, index, products, doc_lengths, k=10)\n",
    "\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import build_index, search_bm25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "search_bm25() missing 2 required positional arguments: 'products_df' and 'doc_lengths'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m q = queries.iloc[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(queries, \u001b[33m\"\u001b[39m\u001b[33miloc\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m queries[\u001b[32m0\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results = \u001b[43msearch_bm25\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m results[:\u001b[32m3\u001b[39m]\n",
      "\u001b[31mTypeError\u001b[39m: search_bm25() missing 2 required positional arguments: 'products_df' and 'doc_lengths'"
     ]
    }
   ],
   "source": [
    "q = queries.iloc[0][\"query\"] if hasattr(queries, \"iloc\") else queries[0]\n",
    "results = search_bm25(q, index, k=10)\n",
    "results[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2: Understanding Embeddings (15 points)\n",
    "\n",
    "### 2a. Load a local model and generate embeddings (5 pts)\n",
    "\n",
    "Use `sentence-transformers` to load a local embedding model and generate embeddings for a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the all-MiniLM-L6-v2 model using SentenceTransformer\n",
    "# Then generate embeddings for each word in the list\n",
    "words = [\"wooden coffee table\", \"oak dining table\", \"red leather sofa\", \"blue area rug\", \"kitchen sink\"]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Print the number of embeddings you generated and the dimension of the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Implement cosine similarity and create a similarity matrix (5 pts)\n",
    "\n",
    "Implement cosine similarity from scratch:\n",
    "\n",
    "$$\\text{cosine\\_similarity}(a, b) = \\frac{a \\cdot b}{\\|a\\| \\times \\|b\\|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement cosine similarity from scratch\n",
    "\n",
    "# Create similarity matrix\n",
    "\n",
    "# Display as DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Embed using OpenAI API (5 pts)\n",
    "\n",
    "Use `litellm` to get embeddings from OpenAI's API and compare dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use litellm to get an embedding from OpenAI's text-embedding-3-small model\n",
    "# Compare the dimension with the local model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 3: Batch Embedding Products (20 points)\n",
    "\n",
    "### 3a. Embed a product sample (10 pts)\n",
    "\n",
    "Create a combined text field and embed 5,000 products using the local model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a consistent sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined text field (product_name + product_class)\n",
    "# Then embed all products using model.encode()\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Save and load embeddings (5 pts)\n",
    "\n",
    "Save embeddings to a `.npy` file so you don't have to recompute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to ../temp/hw4_embeddings.npy\n",
    "# Save products_sample to ../temp/hw4_products.csv\n",
    "# Then load them back and verify they match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Cost estimation (5 pts)\n",
    "\n",
    "Estimate the cost to embed all 43K products using OpenAI's API.\n",
    "\n",
    "**Pricing**: text-embedding-3-small costs ~$0.02 per 1 million tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tiktoken to count actual tokens in the sample\n",
    "# Then extrapolate to estimate cost for the full dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 4: Semantic Search (25 points)\n",
    "\n",
    "### 4a. Implement semantic search (15 pts)\n",
    "\n",
    "Implement a semantic search function from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement batch cosine similarity for efficiency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement semantic search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Evaluate and compare BM25 vs semantic search (10 pts)\n",
    "\n",
    "Implement Recall@k and compare the two search methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Recall@k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build BM25 index for comparison\n",
    "\n",
    "# Filter queries to those with products in our sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both BM25 and semantic search on all queries\n",
    "# Calculate Recall@10 for each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 5: Compare Embedding Models (20 points)\n",
    "\n",
    "### 5a. Embed products with two different models (10 pts)\n",
    "\n",
    "Compare embeddings from:\n",
    "- `BAAI/bge-base-en-v1.5`\n",
    "- `sentence-transformers/all-mpnet-base-v2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the two embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed products with both models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Compare search results between models (10 pts)\n",
    "\n",
    "Evaluate both models on the same queries and analyze differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results for specific queries\n",
    "test_queries = [\"comfortable sofa\", \"star wars rug\", \"modern coffee table\"]\n",
    "# add more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison with a scatter plot\n",
    "# X-axis: BGE Recall@10, Y-axis: MPNet Recall@10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 6: Git Submission (5 points)\n",
    "\n",
    "Submit your work using the Git workflow:\n",
    "\n",
    "- [ ] Create a new branch called `homework-4`\n",
    "- [ ] Commit your work with a meaningful message\n",
    "- [ ] Push to GitHub\n",
    "- [ ] Create a Pull Request\n",
    "- [ ] Merge the PR to main\n",
    "- [ ] Submit the `.ipynb` file on Blackboard\n",
    "\n",
    "The TA will verify your submission by checking the merged PR on GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
